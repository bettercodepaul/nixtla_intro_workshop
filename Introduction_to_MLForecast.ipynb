{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bettercodepaul/nixtla_intro_workshop/blob/main/Introduction_to_MLForecast.ipynb\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLXb0BBRFy7C"
      },
      "source": [
        "# Introduction to Forecasting with Nixtla's mlforecast\n",
        "\n",
        "This notebook walks you through the very basics of forecasting time series with Nixtla's mlforecast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7KgpyEGTs5"
      },
      "source": [
        "## Install and import necessary libraries\n",
        "\n",
        "We use [Polars](https://docs.pola.rs/) for data wrangling, [Plotly](https://plotly.com/python/plotly-express/) for visualizations and Nixtla's [mlforecast](https://nixtlaverse.nixtla.io/mlforecast/index.html) for time series forecasting with machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK4r_pvhQDOi"
      },
      "outputs": [],
      "source": [
        "pip -q install mlforecast polars plotly scikit-learn==1.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWDpn06iP2VY",
        "outputId": "91cae4c7-430c-4908-8b85-10a4d0c8e5c3"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import polars as pl\n",
        "import plotly.express as px\n",
        "from datetime import date\n",
        "from mlforecast import MLForecast\n",
        "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
        "from mlforecast.target_transforms import Differences\n",
        "from utilsforecast.plotting import plot_series\n",
        "\n",
        "pl.Config(tbl_rows=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-iCk7RP3nJ7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-tgF0XGoy7"
      },
      "source": [
        "## Initial Exploration of the data\n",
        "\n",
        "The data for this walk through is monthly sales data for various countries and products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "-qgnbWMtP7w3",
        "outputId": "ca4fb686-160a-47be-9d45-87a2d464bc62"
      },
      "outputs": [],
      "source": [
        "Y_df = pl.read_parquet(\"https://github.com/bettercodepaul/nixtla_intro_workshop/raw/refs/heads/main/retail_sales_product_level.parquet\")\n",
        "Y_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL0YKBwvIDAk"
      },
      "source": [
        "We can visualize the time series with Plotly. The sales volume on the country level is the same as in the introducing notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(Y_df.group_by(pl.col(\"country\").alias(\"unique_id\"), \"ds\").agg(pl.col(\"y\").sum()).sort(\"ds\"), ids=[\"Deutschland\", \"Frankreich\", \"Italien\", \"Grossbritannien\", \"Japan\", \"USA\"], engine=\"plotly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2hvXWOzrtNL"
      },
      "source": [
        "However, in this data we also have sales on the product level. For example we can visualize the sales for all products of type *elegant* and segment *medium*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "zZC_aFdMFTP-",
        "outputId": "0c646784-53c4-4cd7-d91f-89d2e091913b"
      },
      "outputs": [],
      "source": [
        "product_type = \"elegant\" # elegant or comfortable\n",
        "product_segment = \"big\" # small, medium or big\n",
        "df_plot = (\n",
        "    Y_df\n",
        "    .filter(pl.col(\"type\").eq(product_type) & pl.col(\"segment\").eq(product_segment))\n",
        "    .group_by(\"ds\", \"version\")\n",
        "    .agg(pl.col(\"y\").sum())\n",
        "    .sort(\"ds\")\n",
        ")\n",
        "px.line(df_plot, x=\"ds\", y=\"y\", color=\"version\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local Models vs Global Models\n",
        "\n",
        "Classic time series model like ARIMA, Exponential Smoothing and GARCH always operate on single time series. They learn from the past of this single time series to forecast the future.\n",
        "\n",
        "![Local Models](https://github.com/bettercodepaul/nixtla_intro_workshop/blob/main/images/Local%20Models.png?raw=true)\n",
        "\n",
        "Global models on the other hand learn from various time series at once. Relationships and patterns learned from one time series can be transferred to other time series in a global model.\n",
        "\n",
        "Caution: Just because a model is global doesn't mean that the model treats all time series uniformly. It is therefore possible that the global model in our example handles Spain differently than Italy.\n",
        "\n",
        "Global models often also have an advantage in addressing the so-called cold start problem. This is when a new time series needs to be predicted, but no prior data is available for it.\n",
        "\n",
        "![Global Models](https://github.com/bettercodepaul/nixtla_intro_workshop/blob/main/images/Global%20Models.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psTTwljLIqAL"
      },
      "source": [
        "## Transforming the data to be suitable for Machine Learning\n",
        "\n",
        "We need to transform the data to be able to feed it into a Machine Learning algorithm, because these algorithms are built to look at a row with different features and a target value.\n",
        "\n",
        "![Machine Learning Models](https://github.com/bettercodepaul/nixtla_intro_workshop/blob/main/images/Machine%20Learning%20Models.png?raw=true)\n",
        "\n",
        "For time series a classic feature is the lag of the time series. That is the value the time series had in the past. E.g. a lag 1 feature would be the value of the target one time step before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3pX0ysatQbv"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
        "]\n",
        "fcst = MLForecast(\n",
        "    models=models,\n",
        "    freq='1mo',\n",
        "    lags=[1, 12], # create lags for previous month and same month last year\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "e4PQuewLyYdr",
        "outputId": "cc84a009-d94a-4e83-fa4f-6eb4d0442820"
      },
      "outputs": [],
      "source": [
        "# check what the preprocessed data looks like\n",
        "fcst.preprocess(Y_df.select(\"unique_id\", \"ds\", \"y\"), dropna=False).filter(pl.col(\"unique_id\").eq(\"Japan-big-elegant-3\")).sort(\"ds\").head(14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "d2uaUptbzexO",
        "outputId": "47e6570f-7d36-4d42-84dd-79014ab0e358"
      },
      "outputs": [],
      "source": [
        "# Another useful feature is the month so that the model is able to capture the seasonality\n",
        "fcst = MLForecast(\n",
        "    models=models,\n",
        "    freq='1mo',\n",
        "    lags=[1, 12], # create lags for previous month and same month last year\n",
        "    date_features=['month'], # create a feature for the month\n",
        ")\n",
        "fcst.preprocess(Y_df.select(\"unique_id\", \"ds\", \"y\"), dropna=False).filter(pl.col(\"unique_id\").eq(\"Japan-big-elegant-3\")).sort(\"ds\").head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "k25fDsPLz5rr",
        "outputId": "52f119fa-790f-4752-a601-2b96a0c572f8"
      },
      "outputs": [],
      "source": [
        "# rolling and expanding means help to capture the trend of a time series without the model having to reconstruct that from a lot of lags\n",
        "fcst = MLForecast(\n",
        "    models=models,\n",
        "    freq='1mo',\n",
        "    lags=[1, 12], # create lags for previous month and same month last year\n",
        "    lag_transforms={\n",
        "        1: [ExpandingMean(), RollingMean(window_size=4, min_samples=1)],\n",
        "        12: [ExpandingMean()]\n",
        "    },\n",
        "    date_features=['month'], # create a feature for the month\n",
        ")\n",
        "features = fcst.preprocess(Y_df.select(\"unique_id\", \"ds\", \"y\"), dropna=False).filter(pl.col(\"unique_id\").eq(\"Japan-big-elegant-3\")).sort(\"ds\")\n",
        "features.tail(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9lHSW-SF0h6W",
        "outputId": "1ef53fc6-23d4-46ec-96cc-639614b7e8c7"
      },
      "outputs": [],
      "source": [
        "# visualizing the different features helps a lot\n",
        "px.line(features, x=\"ds\", y=[\"y\", \"lag1\", \"lag12\", \"expanding_mean_lag1\", \"rolling_mean_lag1_window_size4_min_samples1\", \"expanding_mean_lag12\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering for Time Series Forecasting\n",
        "Feature engineering is central to the success of machine learning models, especially in the context of time series analysis. As Andrew Ng said: \"Applied Machine Learning is basically Feature Engineering.\" It requires creativity, domain expertise, and time to extract meaningful and informative features from the data. Below is a structured overview of the approaches and methods of feature engineering for time series forecasting.\n",
        "\n",
        "### Datetime-Based Features\n",
        "\n",
        "Capture seasonal and time-dependent patterns:\n",
        "\n",
        "- Yearly Seasonality: Month, quarter, week of the year.\n",
        "- Monthly Seasonality: Week of the month, day of the month.\n",
        "- Weekly Seasonality: Day of the week, weekend indicator.\n",
        "- Daily Timing: Hour, minute, second (e.g., for hourly data).\n",
        "\n",
        "Potential transformations:\n",
        "\n",
        "- Fourier terms (sin/cos) for representing periodic patterns.\n",
        "- Treating time-based features as categorical variables (e.g., days of the week: Mon, Tue).\n",
        "\n",
        "### Event-Based Features\n",
        "\n",
        "Features derived from external or internal events:\n",
        "\n",
        "- Exogenous Events: Holidays, large-scale events, political elections.\n",
        "- Endogenous Events: Promotional campaigns, price changes, product launches or withdrawals.\n",
        "- Recursively Derived Events: Triggers from the time series itself, e.g., crossing a specific threshold.\n",
        "\n",
        "Encoding types:\n",
        "\n",
        "- Counts: Number of holidays in a given month.\n",
        "- Boolean: Is today a holiday (e.g., True/False)?\n",
        "- Time Span: Days until the next event or since the last one.\n",
        "\n",
        "### Forecast vs. Predicted Points\n",
        "\n",
        "Features can be derived based on the time of prediction or the target time being forecasted:\n",
        "\n",
        "Features at the Forecast Time (Current Time):\n",
        "\n",
        "- Is today New Year’s Eve?\n",
        "- Is today Monday?\n",
        "- How many days have passed since a specific event (e.g., the company’s founding, store opening)?\n",
        "\n",
        "Features for the Predicted Time (Future Time):\n",
        "\n",
        "- Will it be New Year’s Eve?\n",
        "- Will it be Monday?\n",
        "- How many days will have passed since a specific event?\n",
        "\n",
        "By distinguishing these two perspectives, you can account for the context at both the prediction point and the target time, which can significantly improve the predictive power of your model.\n",
        "\n",
        "### Lags and Statistical Features\n",
        "\n",
        "- Lags: Past values from the time series.\n",
        "    - Example: Value exactly one month ago (Lag_1), two months ago (Lag_2).\n",
        "- Rolling Statistics: Statistics over a sliding time window e.g. using mean, median, minimum, maximum, or standard deviation.\n",
        "    - Example: Average of the past 7 days.\n",
        "- Expanding Statistics: Statistics over a growing time window.\n",
        "    - Example: Mean from the beginning of observations up to today.\n",
        "\n",
        "Application examples:\n",
        "\n",
        "- Smoothing a time series with a rolling mean.\n",
        "- Detecting changing variance in the data with rolling standard deviation.\n",
        "\n",
        "### Transformations of Time Series\n",
        "\n",
        "- Differencing: Changes between consecutive observations.\n",
        "- Integration: Cumulative sum from the start of observations.\n",
        "- Power/Logarithmic Transformations: (e.g., Box-Cox) for variance stabilization and normalization.\n",
        "\n",
        "### Categorical Features and Encoding\n",
        "\n",
        "Most machine learning algorithms require numerical inputs, so categorical features often need to be encoded:\n",
        "\n",
        "- One-Hot Encoding: Each category is represented by a separate binary column.\n",
        "- Target-Based Encoding: Using the mean/median of the target variable for each category (careful: risk of data leakage!).\n",
        "- Count Encoding: Number of observations per category.\n",
        "- Embeddings: Low-dimensional feature representations, particularly useful for high cardinality.\n",
        "\n",
        "### Interactions and Feature Combinations\n",
        "\n",
        "#### Between Features\n",
        "\n",
        "- Combining existing features\n",
        "    - Example: Price per square meter = Price ÷ Area.\n",
        "    - Example: Covid infections per capita.\n",
        "\n",
        "#### Between Time Series\n",
        "\n",
        "- Interactions between different time series:\n",
        "    - Example: Sum of sales within a product category across different regions.\n",
        "- Aggregations as a summary\n",
        "    - Average values for specific groups.\n",
        "    - Key external time series, e.g., infection rates in neighboring countries.\n",
        "\n",
        "### Iterative Feature Engineering\n",
        "\n",
        "Self-Referencing Features: A newly derived feature can open up possibilities for additional features.\n",
        "\n",
        "Example: Calculate the difference in a time series → Rolling mean of these differences can serve as a trend indicator (similar to the slope of a regression line through the series).\n",
        "\n",
        "Iterative application of transformations on already derived features.\n",
        "\n",
        "### Avoiding Data Leakage\n",
        "\n",
        "A common mistake in feature engineering is the use of information that would not be available at the time of prediction (data leakage). If you leak future data from the training set your model performance will suffer (it will think a feature is better than it actually is). If you leak future data from the validation set your validation performance will suffer (you will think that the model is better than it actually is).\n",
        "\n",
        "Examples include:\n",
        "\n",
        "- Computing statistics (e.g., mean) over the entire time series instead of using a sliding or expanding window.\n",
        "- Encoding methods that unintentionally leak target values into the features.\n",
        "\n",
        "#### Forecasted Features\n",
        "\n",
        "Incorporating forecasted features into your model can significantly enhance its performance. For example, if your target variable depends on weather conditions, including a weather forecast as an input feature can provide valuable context and improve predictive accuracy. However, there is a common pitfall to watch out for in practice:\n",
        "\n",
        "**Big mistake**: Training with actual observed data (e.g. historical weather) but predicting with forecasted data (e.g. a weather forecast).\n",
        "\n",
        "This mistake can invalidate both your model and its validation results for the following reasons:\n",
        "\n",
        "- **Overestimating Feature Utility**: When the model is trained on observed (actual) weather data, it becomes overly confident in the accuracy and reliability of the weather feature. However, in real-world applications, the weather forecast used during prediction may have uncertainty or errors that your model has not accounted for, leading to degraded performance.\n",
        "\n",
        "- **Overestimating Model Performance**: Validation results based on observed weather data will give an overly optimistic measure of model performance. In practice, where forecasted weather data is used during inference, the model is likely to perform worse than indicated by the validation.\n",
        "\n",
        "To address this issue, it becomes necessary to incorporate simulated or historical forecast data during model training and validation. This ensures consistency with the data that will be available during inference. However, this approach complicates the training pipeline because you now need access to historical forecasts and most forecasting libraries do not support this.\n",
        "\n",
        "\n",
        "### Further Inspiration\n",
        "\n",
        "- [Featuretools](https://featuretools.alteryx.com/en/stable/): Automated feature engineering.\n",
        "- [Tsfresh](https://github.com/blue-yonder/tsfresh): Time series-specific feature engineering.\n",
        "- [Feature Engineering and Selection](https://feat.engineering): A practical approach for Predictive Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npkYaFJ9QCA"
      },
      "source": [
        "## Making a prediction with a recursive one-step ahead forecaster\n",
        "\n",
        "To make a prediction we can simply call the fit method. This will create a one-step ahead forecast. A model that can predict the value for the next time step.\n",
        "\n",
        "![Recursive Model](https://github.com/bettercodepaul/nixtla_intro_workshop/blob/main/images/Recursive%20Model.gif?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRZYKrzV_z2t",
        "outputId": "5dc4bdb1-95a4-402e-e017-3a5f8b991d6b"
      },
      "outputs": [],
      "source": [
        "# now that we have prepared the features, we can fit a model\n",
        "fcst.fit(Y_df.select(\"unique_id\", \"ds\", \"y\"), dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFrSu8gl9hW2"
      },
      "source": [
        "Making the actual predictions can then be done using the predict method. This will recursively make the predictions (use the result of the first forecast to create the features for the second forecast)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "nevopZMgtEMz",
        "outputId": "cc8a7bb9-2bb7-4e16-ce7b-2b796d81eb6b"
      },
      "outputs": [],
      "source": [
        "# and make a prediction for one year ahead\n",
        "predictions = fcst.predict(12)\n",
        "predictions.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "qYlMVvuHvtPy",
        "outputId": "a52606c9-a7f4-4428-efca-9b922991b071"
      },
      "outputs": [],
      "source": [
        "# what is interesting: the model predicts all series, also those from the past that are no longer of interest!\n",
        "from utilsforecast.plotting import plot_series\n",
        "plot_series(Y_df, predictions, max_ids=4, plot_random=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "yRvz8CwSvthl",
        "outputId": "874e7315-07a7-4cc1-b407-83d47e2bc62e"
      },
      "outputs": [],
      "source": [
        "# beware of a cross validation in such a case!\n",
        "# You will validate against series from the training window!\n",
        "# THIS IS FUTURE LEAKAGE AT ITS FINEST!\n",
        "cv_result = fcst.cross_validation(\n",
        "    Y_df.select(\"unique_id\", \"ds\", \"y\"),\n",
        "    n_windows=4,  # number of models to train/splits to perform\n",
        "    h=12,  # length of the validation set in each window\n",
        ")\n",
        "cv_result.group_by(\"unique_id\").agg(pl.col(\"cutoff\").unique(), pl.col(\"cutoff\").n_unique().alias(\"n_cutoff\")).sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6IwNI0198iN"
      },
      "source": [
        "We can fill the time series before they start and after they finish to avoid this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "jOJ3Xyiw8S0D",
        "outputId": "eace2ad8-f5e8-45fe-c105-21a80b62d91a"
      },
      "outputs": [],
      "source": [
        "px.line(Y_df.filter(pl.col(\"unique_id\").eq(\"Italien-small-elegant-2\")), x=\"ds\", y=\"y\", title=\"Before filling with 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "i_3iWCQk7unj",
        "outputId": "345f639a-bedc-4827-fac3-13b2ca4ae82b"
      },
      "outputs": [],
      "source": [
        "# adjust the time series to cover the complete range\n",
        "from utilsforecast.preprocessing import fill_gaps\n",
        "Y_df_filled = fill_gaps(Y_df, freq=\"1mo\", start=\"global\", end=\"global\").with_columns(pl.col(\"y\").fill_null(0))\n",
        "px.line(Y_df_filled.filter(pl.col(\"unique_id\").eq(\"Italien-small-elegant-2\")), x=\"ds\", y=\"y\", title=\"After filling with 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Beware that we would have to adjust the features as well because fill_gaps is unaware of static features (country, segment, ...) and how to calculate the non-static features (e.g. months_till_eol, months_till_start)\n",
        "Y_df_filled.filter(pl.col(\"unique_id\").eq(\"Italien-small-elegant-2\")).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "lkWkILcB7u9_",
        "outputId": "42972adc-953b-4a12-d762-9d3726f619c3"
      },
      "outputs": [],
      "source": [
        "cv_result = fcst.cross_validation(\n",
        "    Y_df_filled.select(\"unique_id\", \"ds\", \"y\"),\n",
        "    n_windows=4,  # number of models to train/splits to perform\n",
        "    h=12,  # length of the validation set in each window\n",
        ")\n",
        "cv_result.group_by(\"unique_id\").agg(pl.col(\"cutoff\").unique(), pl.col(\"cutoff\").n_unique().alias(\"n_cutoff\")).sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsoLlNWcAjVj"
      },
      "source": [
        "## Hands-on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filling with zeros to avoid the future leakage works. However, this is also really problematic. What have we done to the distribution of the training and test data? Is it now easier or harder for the model to predict `y`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# room for your thoughts or analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Try to do some feature engineering to improve the model\n",
        "- Play around with different lags and expanding means/rolling means\n",
        "- What is the best value you can get?\n",
        "- If that is boring for you, you can also have a look at the take home assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSHYwzEKDMhA"
      },
      "outputs": [],
      "source": [
        "# copy and modify the code from above that creates the forecasting object fcst = MLForecast(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "rFmYn2QVAiiG",
        "outputId": "2a9c940d-cdee-4e12-a9f5-3b6146fccc5f"
      },
      "outputs": [],
      "source": [
        "from utilsforecast.losses import rmse, mae, mape, bias\n",
        "\n",
        "cv_result = fcst.cross_validation(\n",
        "    Y_df.select(\"unique_id\", \"ds\", \"y\"),\n",
        "    n_windows=4,  # number of models to train/splits to perform\n",
        "    h=12,  # length of the validation set in each window\n",
        ")\n",
        "rmse(cv_result, models=['LGBMRegressor'], id_col='cutoff').select(pl.col(\"LGBMRegressor\").mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJdCdGH_QzN"
      },
      "source": [
        "## Take home assignment\n",
        "\n",
        "\n",
        "You can explore various topics we could not cover today!\n",
        "\n",
        "- Add the static features for segment and type (you need to transform them to a numerical representation, e.g. using `pl.col(\"segment\").cast(pl.Categorical).to_physical()`)\n",
        "- Add features that covers the lifecycle of the products (months since market introduction, months until end of lifecycle). See https://nixtlaverse.nixtla.io/mlforecast/docs/how-to-guides/exogenous_features.html\n",
        "- Add features that calculate the trend of the segment and the type (difficult with Nixtla!)\n",
        "- How could you give the model a hint regarding the level of a new time series (cold-start problem)?\n",
        "- Add features that cover predecessors of the product to be forecasted (which also helps for the cold-start problem)\n",
        "- The recursive approach comes with quite some downsides. Train one model per horizon to get better forecasts https://nixtlaverse.nixtla.io/mlforecast/docs/how-to-guides/one_model_per_horizon.html and check https://medium.com/data-science/the-perils-of-recursive-forecasting-82ebd218d147\n",
        "- Implement a proper rolling cross-validation on your own:\n",
        "    - prepend the individual time series based on the maximum forecast horizon (e.g. for 12 months you would prepend the time series with 12 time steps)\n",
        "    - implement the cross-validation to only forecast time series that are included in the validation window"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
